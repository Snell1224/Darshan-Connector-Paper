\section{Results}
\label{sec:results}

This section covers what significance of the approach to collecting
runtime application I/O data using \Darshan. We present performance
analysis that shows how the new metrics helped provide more insight
results into I/O behavior and how this information can be represented
in Grafana.

\subsection{Experiments and Overhead}
Each application was tested on the Lustre and NFS file system with various configurations for each application run. \RED{HMMER could only run on one node because...} All application experiments were repeated 5 times for the \connector{} and Darshan only (i.e. no LDMS implemented) which summed to a total of 110 job submissions. The layout for these runs are shown in Table~\ref{table:apps}.  

The average of the 5 execution times (e.g. Average Runtime (s)) for Darshan and the \connector{} (e.g. dC) was then taken and used to calculate the percent overhead of LDMS. As seen from Table~\ref{subtable:MPI-IO-TEST}, the overhead of LDMS on Darshans' MPI-IO-TEST benchmark was \RED{briefly compare the overhead across applications}.
\begin{table}[h]
    \begin{subtable}[h]{0.5\textwidth}
    \vspace{0.5cm}
        \centering
        \setlength\tabcolsep{8pt}
       \begin{tabular}{|ccccc|}
        \hline
        \multicolumn{5}{|c|}{MPI-IO-TEST}                                                                                                              \\ \hline
        \multicolumn{1}{|c|}{File System}        & \multicolumn{2}{c|}{NFS}                                    & \multicolumn{2}{c|}{Lustre}           \\ \hline
        \multicolumn{1}{|c|}{Nodes}              & \multicolumn{2}{c|}{22}                                     & \multicolumn{2}{c|}{22}               \\ \hline
        \multicolumn{1}{|c|}{Block Size}         & \multicolumn{2}{c|}{16*1024*1024}                           & \multicolumn{2}{c|}{16*1024*1024}     \\ \hline
        \multicolumn{1}{|c|}{Iterations}         & \multicolumn{2}{c|}{10}                                     & \multicolumn{2}{c|}{10}               \\ \hline
        \multicolumn{1}{|c|}{Collective}         & \multicolumn{1}{c|}{Yes}     & \multicolumn{1}{c|}{No}      & \multicolumn{1}{c|}{Yes}    & No      \\ \hline
        \multicolumn{1}{|c|}{Avg. Messages}      & \multicolumn{1}{c|}{50390}   & \multicolumn{1}{c|}{6397}    & \multicolumn{1}{c|}{25770}  & 15676   \\ \hline
        \multicolumn{1}{|c|}{Rate (msgs/sec)} & \multicolumn{1}{c|}{37}      & \multicolumn{1}{c|}{7}       & \multicolumn{1}{c|}{95}     & 38      \\ \hline
        \multicolumn{5}{|c|}{Average Runtime (s)}                                                                                                      \\ \hline
        \multicolumn{1}{|c|}{Darshan}            & \multicolumn{1}{c|}{1376.67} & \multicolumn{1}{c|}{880.46}  & \multicolumn{1}{c|}{249.97} & 428.18  \\ \hline
        \multicolumn{1}{|c|}{dC}                 & \multicolumn{1}{c|}{1355.35} & \multicolumn{1}{c|}{858.68}  & \multicolumn{1}{c|}{270.98} & 414.35  \\ \hline
        \multicolumn{1}{|c|}{\% Overhead}        & \multicolumn{1}{c|}{-1.55\%} & \multicolumn{1}{c|}{-2.47\%} & \multicolumn{1}{c|}{8.41\%} & -3.23\% \\ \hline
        \end{tabular}
    \caption{MPI-IO} 
    \label{subtable:mpi-io-test}
    \vspace{0.5cm}
    \end{subtable}
    \begin{subtable}[h]{0.5\textwidth}
        \centering
        \setlength\tabcolsep{5.5pt}
        \begin{tabular}{|ccccc|}
        \hline
        \multicolumn{5}{|c|}{HACC-IO}                                                                                                                   \\ \hline
        \multicolumn{1}{|c|}{File System}     & \multicolumn{2}{c|}{NFS}                                      & \multicolumn{2}{c|}{Lustre}             \\ \hline
        \multicolumn{1}{|c|}{Nodes}           & \multicolumn{2}{c|}{16}                                       & \multicolumn{2}{c|}{16}                 \\ \hline
        \multicolumn{1}{|c|}{Particles/Rank}  & \multicolumn{1}{c|}{5000000}  & \multicolumn{1}{c|}{10000000} & \multicolumn{1}{c|}{5000000} & 10000000 \\ \hline
        \multicolumn{1}{|c|}{Avg. Messages}   & \multicolumn{1}{c|}{1663}     & \multicolumn{1}{c|}{1774}     & \multicolumn{1}{c|}{1995}    & 1711     \\ \hline
        \multicolumn{1}{|c|}{Rate (msgs/sec)} & \multicolumn{1}{c|}{2}        & \multicolumn{1}{c|}{1}        & \multicolumn{1}{c|}{3}       & 2        \\ \hline
        \multicolumn{5}{|c|}{Average Runtime (s)}                                                                                                       \\ \hline
        \multicolumn{1}{|c|}{Darshan}         & \multicolumn{1}{c|}{882.46}   & \multicolumn{1}{c|}{1353.87}  & \multicolumn{1}{c|}{417.14}  & 1616.87  \\ \hline
        \multicolumn{1}{|c|}{dC}              & \multicolumn{1}{c|}{775.24}   & \multicolumn{1}{c|}{1365.24}  & \multicolumn{1}{c|}{467.24}  & 1027.44  \\ \hline
        \multicolumn{1}{|c|}{\% Overhead}     & \multicolumn{1}{c|}{-12.15\%} & \multicolumn{1}{c|}{0.84\%}   & \multicolumn{1}{c|}{12.01\%} & -36.45\% \\ \hline
        \end{tabular}
    \caption{HACC-IO} 
    \label{subtable:HACC}
    \vspace{0.5cm}
    \end{subtable}
    \begin{subtable}[h]{0.5\textwidth}
        \centering
        \setlength\tabcolsep{18.5pt}
        \begin{tabular}{|cclcl|}
        \hline
        \multicolumn{5}{|c|}{HMMER}                                                                            \\ \hline
        \multicolumn{1}{|c|}{File System}     & \multicolumn{2}{c|}{NFS}      & \multicolumn{2}{c|}{Lustre}    \\ \hline
        \multicolumn{1}{|c|}{Nodes}           & \multicolumn{2}{c|}{1}        & \multicolumn{2}{c|}{1}         \\ \hline
        \multicolumn{1}{|c|}{Input}           & \multicolumn{4}{c|}{Pfam-A.seed}                               \\ \hline
        \multicolumn{1}{|c|}{Avg. Messages}   & \multicolumn{2}{c|}{3117342}  & \multicolumn{2}{c|}{4461738}   \\ \hline
        \multicolumn{1}{|c|}{Rate (msgs/sec)} & \multicolumn{2}{c|}{1483}     & \multicolumn{2}{c|}{2396}      \\ \hline
        \multicolumn{5}{|c|}{Average Runtime (s)}                                                              \\ \hline
        \multicolumn{1}{|c|}{Darshan}         & \multicolumn{2}{c|}{749.88}   & \multicolumn{2}{c|}{135.40}    \\ \hline
        \multicolumn{1}{|c|}{dC}              & \multicolumn{2}{c|}{2826.01}  & \multicolumn{2}{c|}{1863.98}   \\ \hline
        \multicolumn{1}{|c|}{\% Overhead}     & \multicolumn{2}{c|}{276.86\%} & \multicolumn{2}{c|}{1276.67\%} \\ \hline
        \end{tabular}
    \caption{HMMER} 
    \label{subtable:HMMER}
\end{subtable}

\caption{Overview of each experiment configuration, target file system, average elapsed time (s) from 5 runs and calculated overhead of LDMS.}
\label{table:apps}
\end{table}

\subsection{Analysis and Grafana Output}
Figure \ref{f:hacc} presents the mean number of I/O operations for
each HACC application and the error bar considering 95\% confidence
interval for the five jobs. This plot shows that even running at the
same system and with the same configuration, the applications
presented different I/O behavior. In fact, a single HPC application
can have multiple unique I/O behavior that can degradate the
performance of the application \cite{costa2021}. This I/O variation
can also happen between allocated devices. Figure \ref{f:hacc2} shows
the number of I/O requests per node for close and open operations for
two jobs for the HACC-IO application on Lustre for 10 millions
particles per rank.

\begin{figure}
	\centering
	% \includegraphics[width=\linewidth]{figs/255653_mpi_io_luster_no_coll_duration.pdf}
        \includegraphics[width=\linewidth]{figs/operations_hacc.pdf}
	\caption{The same application can perform different amount of
          I/O operations during execution. It shows the mean
          occurences of each operation over the five job runs.}
	\label{f:hacc}
\end{figure}

% jobs "255515", "255675"
\begin{figure}
	\centering
        \includegraphics[width=\linewidth]{figs/hacc_nfs_10.pdf}
	\caption{The same application can perform different amount of
          I/O operations per node.}
	\label{f:hacc2}
\end{figure}

Figure \ref{f:mpi_io_all} shows the duration of the reads and writes
per rank for each execution (\texttt{job\_id} metric) of the MPI-IO
benchmark without using collective operations. We notice a similar
behavior for the I/O operations duration for all jobs except the
second one (\texttt{job\_id 2}). It presents a mean duration of 6.75
seconds for reads and 78s for writes, while the other jobs had a mean
duration of 0.05s for reads and 54s for writes. With the collected
logs, we can perform a spatial performance analysis to understand the
variability in the I/O behavior per system component, in this case,
per nodes and ranks.

\begin{figure}
	\centering
	% \includegraphics[width=\linewidth]{figs/255653_mpi_io_luster_no_coll_duration.pdf}
        \includegraphics[width=\linewidth]{figs/mpi_io_luster_no_coll_duration_allexperiments.pdf}
	\caption{Jobs for the MPI-IO benchmark without collective
          operations presented variability in the number and duration
          of I/O operations.}
	\label{f:mpi_io_all}
\end{figure}

Using the absolute timestamps collected we can temporarily view
wherein the application execution the variability of a job
occured. Figure \ref{f:mpi_io} presents the duration and occurence of
I/O operations throughout the MPI-IO benchmark for \texttt{job\_id
  2}. We can identify the application I/O pattern of performing
writings during ten phases, and then reads at the end. Also, this
application run faster writes at the beginning and slower at the end,
with the slowest writting after 250 seconds.
      
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figs/255653_mpi_io_luster_no_coll_execution2.pdf}
	\caption{Distribution of reads and writes operations
          throughout the execution time for the \texttt{job\_id 2},
          can reveal the application I/O pattern, and wherein the
          application there were faster and slower operations.}
	\label{f:mpi_io}
\end{figure}

The same job is also represented in Figure \ref{f:mpi_io_grafana}
using the Grafana interface. This figure presents the number of I/O
writes (blues) and reads (green) and the amount of bytes sent
aggregated across ranks. The writes behavior shows the application
phases where it dealt with larger I/O sizes, with two moments writting
more than 20GB, while the reading operations run for a shorter moment
for aroun 12GB of I/O size. Grafana offers an interactive front-end
view where users can easily filter to visualize specific time and
metrics intervals. Such representation using the absolute timestamps
facilitates the correlation of I/O performance congestion, for
example, with system behavior monitoring, which can also be
represented as a Grafana dashboard.

\begin{figure*}[h!]
	\centering
	\includegraphics[width=\textwidth]{figs/255653_mpi_io_luster_no_coll.pdf}
	\caption{Graphana visualization of the \texttt{job\_id 2}
          writes (blue) and reads (green) operations and amount of bytes per
          operation, using the absolute timestamp metric collected
          with \emph{Darshan LDMS Integration}. Given the other metrics collected by the \connector, many more analyses and visualizations can be made to allow for further insights in application I/O behavior.}
	\label{f:mpi_io_grafana}
      \end{figure*}
