\section{Experimental Methodology}\label{sec:methodology}
This section presents our experimental methodology to evaluate our
framework using four applications with different I/O behavior:
HACC-IO, HMMER, Darshan MPI-IO benchmark, and the sw4 scientific
application. We performed the experiments in a Cray HPC cluster using
NFS and Lustre file systems.

\subsection{Applications}
\begin{itemize}
	\item HACC-IO is the I/O proxy for the large scientific Hardware Accelerated Cosmology Code (HACC), an N-body framework that simulates the evolution of mass in the universe with short and long-range interactions~\cite{habib2013hacc}. The long-range solvers implement an underlying 3D FFT. HACC-IO is an MPI code that simulates the POSIX, MPI collective, and MPI independent I/O patterns of fault tolerance HACC checkpoints. It takes a number of particles per rank as input, writes out a simulated checkpoint information into a file, and then read it for validation. We ran HACC-IO with several configurations to simulate different workloads on the NFS and Lustre file systems. Table~\ref{subtable:HACC} shows the different run configurations. 
	\item HMMER is a suite of applications that profiles a hidden Markov model (HMM) to search similar protein sequences~\cite{eddy1992hmmer}. HMMER has a building code called "hmmbuild" that uses MPI to build a database by concatenating multiple profiles Stockholm alignment files. In our experiment, we used the Pfam-A.seed~\cite{sonnhammer1998pfam} file to generate a large Pfam-A.hmm database. We ran HMMER with 32 MPI ranks on one node, and we ran it in two configurations where we point the database file to NFS and then Lustre, respectively. 
	\item MPI-IO-TEST benchmark is a Darshan utility that exists in the code distribution to test the MPI I/O performance on HPC machines. It can produce iterations of messages with different block sizes sent from various MPI ranks. It can also simulate collective and independent MPI I/O methods. We experimented with NFS vs. Lustre and collective vs. independent MPI I/O. We ran the benchmark with four configurations on 22 nodes and set the number of iterations to 10 and the block size to 16MB. Table~\ref{table:mpi-io-test} shows the different configuration used.
	\item sw4 is a geodynamics code that solves 3D seismic wave equations with local meshrefinement~\cite{peterssonsw4}. sw4 accepts an input file that specifies the 3D grid simulation size, and we selected a size that uses about 50\% of the available memory to memic a realistic run of the application.
\end{itemize}
%provides various use cases of the \Darshan timeseries data that will be used to create new meaningful analyses and insights in the I/O performance variability during an application run.

\subsection{Evaluation System}
We experiment using several I/O loads on the Voltrino Cray XC40 system at Sandia National Laboratories. The system has 24  diskless nodes with Dual Intel Xeon Haswell E5-2698 v3 @ 2.30GHz 16 cores, 32 threads/socket, 64 GB DDR3-1866MHz memory, and connected with a Cray Aries DragonFly interconnect. The machine has two file systems: the network file system (NFS) and the Lustre file system (LFS).\todo{File systems???}

\subsection{Enviroment}
Voltrino, run LDMS samplers on the compute nodes and one LDMS aggregator on the head node. LDMS uses the UGNI interface to transfer Darshan streams data and other performance metrics from the compute nodes to the head node. The aggregator on the head node transmits the data to another LDMS aggregator on another cluster, Shirley, for analysis and storage. Shirley, host the HPC web services Grafana application and the DSOS database. Darshan can wrap the I/O function in an application by linking the executables staticky and dynamic. Our framework uses dynamic linking to collect darshan data. So we set the \code{LD\_Preload} environment variable to point the path of the Darshan library shared objects which have the LDMS streams API calls to send the data through the LDMS compute node daemons. 
\begin{table}[h]
    \begin{subtable}[h]{0.45\textwidth}
    \vspace{0.5cm}
        \centering
        \setlength\tabcolsep{5pt}
        \begin{tabular}{|ccccc|}
        \hline
        \multicolumn{5}{|c|}{MPI-IO-TEST}                                                                                                                                \\ \hline
        \multicolumn{1}{|c|}{File System}      & \multicolumn{2}{c|}{NFS}                                              & \multicolumn{2}{c|}{Lustre}                     \\ \hline
        \multicolumn{1}{|c|}{Block Size}       & \multicolumn{2}{c|}{16*1024*1024}                                     & \multicolumn{2}{c|}{16*1024*1024}               \\ \hline
        \multicolumn{1}{|c|}{Iterations}       & \multicolumn{2}{c|}{10}                                               & \multicolumn{2}{c|}{10}                         \\ \hline
        \multicolumn{1}{|c|}{Nodes}            & \multicolumn{2}{c|}{22}                                               & \multicolumn{2}{c|}{22}                         \\ \hline
        \multicolumn{1}{|c|}{Collective}       & \multicolumn{1}{c|}{Yes}          & \multicolumn{1}{c|}{No}           & \multicolumn{1}{c|}{Yes}         & No           \\ \hline
        \multicolumn{5}{|c|}{Average Runtime (s)}                                                                                                                            \\ \hline
        \multicolumn{1}{|c|}{Darshan}          & \multicolumn{1}{c|}{1376.67}  & \multicolumn{1}{c|}{880.46}  & \multicolumn{1}{c|}{249.97} & 428.18  \\ \hline
        \multicolumn{1}{|c|}{dC} & \multicolumn{1}{c|}{1355.35}  & \multicolumn{1}{c|}{858.68}  & \multicolumn{1}{c|}{270.98} & 414.35  \\ \hline
        \multicolumn{1}{|c|}{\% Overhead}      & \multicolumn{1}{c|}{-0.78\%} & \multicolumn{1}{c|}{-1.25\%} & \multicolumn{1}{c|}{4.03\%} & -1.64\% \\ \hline
        \end{tabular}
    \caption{MPI-IO} 
    \label{subtable:MPI-IO-TEST}
    \vspace{0.5cm}
    \end{subtable}
    \begin{subtable}[h]{0.45\textwidth}
        \centering
        \setlength\tabcolsep{3pt}
        \begin{tabular}{|ccccc|}
        \hline
        \multicolumn{5}{|c|}{HACC-IO}                                                                                                                                  \\ \hline
        \multicolumn{1}{|c|}{File System}      & \multicolumn{2}{c|}{NFS}                                            & \multicolumn{2}{c|}{Lustre}                     \\ \hline
        \multicolumn{1}{|c|}{Nodes}            & \multicolumn{2}{c|}{16}                                             & \multicolumn{2}{c|}{16}                         \\ \hline
        \multicolumn{1}{|c|}{Particles/Rank}   & \multicolumn{1}{c|}{5000000}     & \multicolumn{1}{c|}{10000000}    & \multicolumn{1}{c|}{5000000}     & 10000000     \\ \hline
        \multicolumn{5}{|c|}{Average Runtime (s)}                                                                                                                          \\ \hline
        \multicolumn{1}{|c|}{Darshan}          & \multicolumn{1}{c|}{882.46} & \multicolumn{1}{c|}{1353.87} & \multicolumn{1}{c|}{417.14} & 1616.87  \\ \hline
        \multicolumn{1}{|c|}{dC} & \multicolumn{1}{c|}{775.24} & \multicolumn{1}{c|}{1365.24} & \multicolumn{1}{c|}{467.24} & 1027.44  \\ \hline
        \multicolumn{1}{|c|}{\% Overhead}      & \multicolumn{1}{c|}{-6.47\%} & \multicolumn{1}{c|}{0.42\%} & \multicolumn{1}{c|}{5.67\%}  & -22.29\% \\ \hline
        \end{tabular}
    \caption{HACC-IO} 
    \label{subtable:HACC}
    \vspace{0.5cm}
    \end{subtable}
    
    \begin{subtable}[h]{0.22\textwidth}
        %\centering
        \setlength\tabcolsep{3pt}
        \begin{tabular}{|cclcl|}
        \hline
        \multicolumn{5}{|c|}{HMMER}                                                                                  \\ \hline
        \multicolumn{1}{|c|}{File System}      & \multicolumn{2}{c|}{NFS}         & \multicolumn{2}{c|}{Lustre}      \\ \hline
        \multicolumn{1}{|c|}{Arguments}        & \multicolumn{4}{c|}{with mpi}                                       \\ \hline
        \multicolumn{1}{|c|}{Nodes}            & \multicolumn{2}{c|}{1}           & \multicolumn{2}{c|}{1}           \\ \hline
        \multicolumn{5}{|c|}{Average Runtime (s)}                                                                        \\ \hline
        \multicolumn{1}{|c|}{Darshan}          & \multicolumn{2}{c|}{749.88} & \multicolumn{2}{c|}{135.40} \\ \hline
        \multicolumn{1}{|c|}{dC} & \multicolumn{2}{c|}{?}           & \multicolumn{2}{c|}{?}           \\ \hline
        \multicolumn{1}{|c|}{\% Overhead}      & \multicolumn{2}{c|}{?}           & \multicolumn{2}{c|}{?}           \\ \hline
        \end{tabular}
    \caption{HMMER} 
    \label{subtable:HMMER}
%\end{table}
\end{subtable}
    \vspace{0.5cm}
    \begin{subtable}[h]{0.2\textwidth}
        %\centering
        \setlength\tabcolsep{2pt}
        \begin{tabular}{|ccl|}
        \hline
        \multicolumn{3}{|c|}{SW4}                                                       \\ \hline
        \multicolumn{1}{|c|}{File System}      & \multicolumn{2}{c|}{NFS}               \\ \hline
        \multicolumn{1}{|c|}{Arguments}        & \multicolumn{2}{c|}{new\_gh\_1node.in} \\ \hline
        \multicolumn{1}{|c|}{Nodes}            & \multicolumn{2}{c|}{16}                \\ \hline
        \multicolumn{3}{|c|}{Average Runtime (s)}                                           \\ \hline
        \multicolumn{1}{|c|}{Darshan}          & \multicolumn{2}{c|}{576.86}       \\ \hline
        \multicolumn{1}{|c|}{dC} & \multicolumn{2}{c|}{572.72}       \\ \hline
        \multicolumn{1}{|c|}{\% Overhead}      & \multicolumn{2}{c|}{0.00\%}      \\ \hline
        \end{tabular}
    \caption{SW4} 
    \label{subtable:SW4}
\end{subtable}
\caption{Overview of each experiment configuration, target file system, average elapsed time (s) from 5 runs and calculated overhead of LDMS.}
\label{table:apps}
\end{table}

\subsection{Experiments and Overhead}
Each application was tested on the Lustre and NFS file system with various configurations for each application run. Sw4 was not tested on Lustre because....\RED{why did we not test on lustre? HMMER could only run on one node because...} All application experiments were repeated 5 times for the \connector{} and Darshan only (i.e. no LDMS implemented) which summed to a total of 110 job submissions. The layout for these runs are shown in Table~\ref{table:apps}.  



The average of the 5 execution times (e.g. Average Runtime (s)) for Darshan and the \connector{} (e.g. dC) was then taken and used to calculate the percent overhead of LDMS. As seen from Table~\ref{subtable:MPI-IO-TEST}, the overhead of LDMS on Darshans' MPI-IO-TEST benchmark was \RED{briefly compare the overhead across applications}.

%\RED{Explain the different scenarios we will be testing:
%	\begin{itemize}
%		\item Applications: SWFFT, sw4, sw4lite. Standard baseline: mpi-test. 
%		\item Explain what each application does, why it's being tested and how the test was performed (i.e. number of nodes, etc.)
%		\item Explain the analysis used to analyze the I/O data and how they provide further insight into the I/O behavior and can allow for correlations between I/O performance and system behavior.
%		\item show Grafana graphs, Darshan output (maybe) JSON and any tables (if applicable).
%\end{itemize}}

%\renewcommand{\arraystretch}{1.2}
%\begin{table}[]
%	\centering
%	\begin{tabular}{|c|c|c|c|c|}
%		\hline
%		Application Name & File System	&  Problem Size	& Runtime (seconds) &	Nodes \\ \hline
%		\multirow{4}{*}{HACC-IO} & \multirow{2}{*}{NFS}	& 1000000	& 1210.06 &	\multirow{4}{*}{16} \\ \cline{3-4} 
%		&& 2000000	& 2455.60 &	\\ \cline{2-4}
%		& \multirow{2}{*}{Luster}	& 1000000	&  1476.64 & \\ \cline{3-4} 
%		&& 2000000	&  ?? &	 \\ \hline
		
%		\multirow{4}{*}{MPI-IO-TEST} & \multirow{2}{*}{NFS}	& 16	& 1210.06 &	\multirow{4}{*}{16} \\ \cline{3-4} 
%		&& 16	& 2455.60 &	\\ \cline{2-4}
%		& \multirow{2}{*}{Luster}	& 16	&  1476.64 & \\ \cline{3-4} 
%		&& 16	&  ?? &	 \\ \hline
%
%	\end{tabular}
%	\caption{Applications run configurations, targeted file system, and runtime}
%	\label{table:all-apps}
%\end{table}


%\begin{table}[]
%	\centering
%	\begin{tabular}{|c|c|c|c|}
%		\hline
%		File System	& Particles/Rank	& Runtime (seconds) &	Nodes \\ \hline
%		\multirow{2}{*}{NFS}	& 1000000	& 1210.06 &	16\\ \cline{2-4} 
%		& 2000000	& 2455.60 &	16\\ \hline
%		\multirow{2}{*}{Luster}	& 1000000	&  1476.64 &16 \\ \cline{2-4} 
%		& 2000000	&  ?? &	16 \\ \hline
%	\end{tabular}
%	\caption{HACC-IO run configurations, targeted file system, and runtime}
%	\label{table:HACC}
%\end{table}

%\begin{table}[]
%	\centering
%	\begin{tabular}{|c|c|c|c|}
%		\hline
%		File System	& Block Size (MB)	& Runtime (seconds) &	Nodes \\ \hline
%		\multirow{2}{*}{NFS}	& \multirow{2}{*}{16}	& 1355.35 &	22\\ \cline{3-4} 
%		& 	& ?? &	22\\ \hline
%		\multirow{2}{*}{Luster}	& \multirow{2}{*}{16}	&  270.98 & 22 \\ \cline{3-4} 
%		& 	&  414.34 &	22 \\ \hline
%	\end{tabular}
%	\renewcommand{\arraystretch}{1}
%	\caption{MPI-IO-TEST run configurations, targeted file system, and runtime}
%	\label{table:mpi-io-test}
%\end{table}
